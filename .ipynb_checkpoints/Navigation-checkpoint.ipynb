{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from dqn_agent import Agent\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# please do not modify the line below\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=brain.vector_observation_space_size, action_size=brain.vector_action_space_size)\n",
    "\n",
    "def dqn_train(max_episodes=1000,max_t=1000,eps_start=1.0,eps_end=1e-2,eps_decay=.995):\n",
    "        eps = eps_start\n",
    "        scores = []\n",
    "        score_window = deque(maxlen=100)\n",
    "        avg_scores = []\n",
    "        \n",
    "        for episode_count in range(max_episodes):\n",
    "            \n",
    "            # reset env state\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            state = env_info.vector_observations[0]\n",
    "            \n",
    "            done = env_info.local_done[0]\n",
    "            total_reward = 0\n",
    "            for t in range(max_t):\n",
    "                # choose action using the local q-network\n",
    "                action = agent.act(state,eps)\n",
    "\n",
    "                # taking action in the env\n",
    "                env_info = env.step(vector_action=action)[brain_name]\n",
    "\n",
    "                # getting next_state,reward,done from the env\n",
    "                next_state = env_info.vector_observations[0]\n",
    "                reward = env_info.rewards[0]\n",
    "                done = env_info.local_done[0]\n",
    "                \n",
    "                # using agent to perform a learning step and save the sample in the memory\n",
    "                agent.step(state,action,reward,next_state,done)\n",
    "                \n",
    "                # decay the epsilon value\n",
    "                eps = max(eps_decay*eps,eps_end)\n",
    "                \n",
    "                # get total reward for the episode\n",
    "                total_reward += reward\n",
    "                \n",
    "                # set current state = next_state\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "            scores.append(total_reward)\n",
    "            score_window.append(total_reward)\n",
    "            avg_scores.append(np.mean(score_window))\n",
    "\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode_count, np.mean(score_window)), end=\"\")\n",
    "            if episode_count % 100 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode_count, np.mean(score_window)))\n",
    "            \n",
    "            if np.mean(score_window) > 13:\n",
    "                print('\\rLearning completed in {} episodes ... avg_score :{}'.format(episode_count, np.mean(score_window)))\n",
    "                break\n",
    "                \n",
    "        return scores,avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: -1.00\n",
      "Episode 100\tAverage Score: 1.15\n",
      "Episode 200\tAverage Score: 4.35\n",
      "Episode 300\tAverage Score: 8.50\n",
      "Episode 400\tAverage Score: 10.89\n",
      "Episode 499\tAverage Score: 12.05"
     ]
    }
   ],
   "source": [
    "# training the dqn\n",
    "scores,avg_scores = dqn_train(max_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the avg reward curve\n",
    "plt.plot(range(len(avg_scores)),avg_scores)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('avg score for last 100 episodes')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
