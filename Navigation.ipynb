{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from dqn_agent import Agent\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# loading the unity environment and setting the brain game\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "\n",
    "## headless unity environment\n",
    "#env = UnityEnvironment(file_name=\"Banana_Linux_NoVis/Banana.x86_64\")\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the agent\n",
    "agent = Agent(state_size=brain.vector_observation_space_size, action_size=brain.vector_action_space_size,\n",
    "             lr=1e-4)\n",
    "\n",
    "# training function\n",
    "def dqn_train(max_episodes=1000,max_t=1000,eps_start=1.0,eps_end=1e-2,eps_decay=.995):\n",
    "    \"\"\"\n",
    "    trains the DQN agent\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "    max_episodes : max number of episodes for training the agent\n",
    "    max_t : max number of timesteps for each episode to be used in training\n",
    "    eps_start : starting value of epsilon\n",
    "    eps_end : final value of epsilon\n",
    "    eps_decay : epsilon deacy rate\n",
    "\n",
    "    \"\"\"\n",
    "    eps = eps_start\n",
    "    scores = []\n",
    "    score_window = deque(maxlen=100)\n",
    "    avg_scores = []\n",
    "\n",
    "    for episode_count in range(1,max_episodes+1):\n",
    "\n",
    "        # reset env state\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "\n",
    "        done = env_info.local_done[0]\n",
    "        total_reward = 0\n",
    "        for t in range(max_t):\n",
    "            # choose action using the local q-network\n",
    "            action = agent.act(state,eps)\n",
    "\n",
    "            # taking action in the env\n",
    "            env_info = env.step(vector_action=action)[brain_name]\n",
    "\n",
    "            # getting next_state,reward,done from the env\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "\n",
    "            # using agent to perform a learning step and save the sample in the memory\n",
    "            agent.step(state,action,reward,next_state,done)\n",
    "\n",
    "            # decay the epsilon value\n",
    "            eps = max(eps_decay*eps,eps_end)\n",
    "\n",
    "            # get total reward for the episode\n",
    "            total_reward += reward\n",
    "\n",
    "            # set current state = next_state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(total_reward)\n",
    "        score_window.append(total_reward)\n",
    "        avg_scores.append(np.mean(score_window))\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode_count, np.mean(score_window)), end=\"\")\n",
    "        if episode_count % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode_count, np.mean(score_window)))\n",
    "\n",
    "        if np.mean(score_window) > 13:\n",
    "            print('\\rLearning completed in {} episodes ... avg_score :{}'.format(episode_count, np.mean(score_window)))\n",
    "            torch.save(agent.qnet_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "\n",
    "    return agent,scores,avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training the dqn\n",
    "trained_agent,scores,avg_scores = dqn_train(max_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the avg reward curve\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(len(avg_scores)),avg_scores)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('avg score for last 100 episodes')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained agent weights\n",
    "torch.save(agent.qnet_local.state_dict(),\"checkpoint_local.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this cell to load trained agent weights\n",
    "# initialising the agent\n",
    "trained_agent = Agent(state_size=brain.vector_observation_space_size, action_size=brain.vector_action_space_size)\n",
    "trained_agent.qnet_local.load_state_dict(torch.load('checkpoint_local.pth',map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward : 16.00\n"
     ]
    }
   ],
   "source": [
    "# lets visualise at how the trained agent performs\n",
    "\n",
    "# reset env state\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "state = env_info.vector_observations[0]\n",
    "\n",
    "done = env_info.local_done[0]\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    # choose action using the local q-network using low value of epsilon = 1e-2\n",
    "    action = trained_agent.act(state,eps=1e-2)\n",
    "\n",
    "    # taking action in the env\n",
    "    env_info = env.step(vector_action=action)[brain_name]\n",
    "\n",
    "    # getting next_state,reward,done from the env\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "    \n",
    "    # get total reward for the episode\n",
    "    total_reward += reward\n",
    "    \n",
    "    # set current state = next_state\n",
    "    state = next_state\n",
    "    \n",
    "print (\"total reward : {:.2f}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv3",
   "language": "python",
   "name": "cv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
