{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(\"Banana.app\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork(nn.Module):\n",
    "    def __init__(self,state_size,action_size):\n",
    "        # initialising the super class properties\n",
    "        super(Qnetwork,self).__init__()\n",
    "        \n",
    "        # defining layers\n",
    "        self.fc1 = nn.Linear(state_size,32)\n",
    "        self.fc2 = nn.Linear(32,64)\n",
    "        self.fc3 = nn.Linear(64,128)\n",
    "        self.out = nn.Linear(128,action_size)\n",
    "        \n",
    "    def forward(self,states):\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = F.relu(self.out(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from collections import deque,namedtuple\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,buffer_size,batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        e = self.experience(state,action,reward,next_state,done)\n",
    "        self.buffer.append(e)\n",
    "    \n",
    "    def sample(self,random=True):\n",
    "        if random:\n",
    "            choose = np.random.choice(range(len(self.buffer)),self.batch_size,replace=False)\n",
    "            experiences = [self.buffer[i] for i in choose]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_EVERY = 4\n",
    "class Agent:\n",
    "    def __init__(self,state_size,action_size,gamma=0.99,lr=5e-4,\n",
    "                     buffer_size=int(1e5),batch_size=64,tau=1e-3):\n",
    "        # defining local and target networks\n",
    "        self.qnet_local = Qnetwork(state_size,action_size).to(device)\n",
    "        self.qnet_target = Qnetwork(state_size,action_size).to(device)\n",
    "        \n",
    "        # set local and target parameters equal to each other\n",
    "        self.soft_update(tau=1.0)\n",
    "        \n",
    "        # experience replay buffer\n",
    "        self.memory = ReplayBuffer(buffer_size,batch_size)\n",
    "        \n",
    "        # defining variables\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.t_step = 0\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.qnet_local.parameters(),lr=self.lr)\n",
    "    \n",
    "    def step(self,state,action,reward,next_state,done):\n",
    "        \"\"\" saves the step info in the memory buffer and perform a learning iteration\n",
    "        Input : \n",
    "            state,action,reward,state,done : non-batched numpy arrays\n",
    "        \n",
    "        Output : \n",
    "            none\n",
    "        \"\"\"\n",
    "        # add sample to the memory buffer\n",
    "        self.memory.add(state,action,reward,next_state,done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        \n",
    "        # use replay buffer to learn if it has enough samples\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "        \n",
    "    def learn(self,experiences):\n",
    "        \"\"\" perform a learning iteration by using sampled experience batch\n",
    "        Input : \n",
    "            experience : tuple from the memory buffer\n",
    "            states, actions, rewards, next_states, dones = experiences\n",
    "            eg : states.shape = [N,state_size]\n",
    "        Output : \n",
    "            none\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # predicted action value\n",
    "        q_pred = self.qnet_local.forward(states).gather(1,actions)\n",
    "        \n",
    "        # target action value\n",
    "        q_target = rewards + self.gamma*(1-dones)*self.qnet_target.forward(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # defining loss\n",
    "        loss = F.mse_loss(q_pred,q_target)\n",
    "        \n",
    "        # running backprop and optimizer step\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # run soft update\n",
    "        self.soft_update(self.tau)\n",
    "        \n",
    "    def act(self,state,eps=0.):\n",
    "        \"\"\" return the local model's predicted action for the given state\n",
    "        Input : \n",
    "            state : [state_size]\n",
    "        \n",
    "        Output : \n",
    "            action : scalar action as action space is discrete with dim = 1\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device) # converts numpy array to torch tensor\n",
    "        \n",
    "        self.qnet_local.eval() # put net in test mode\n",
    "        with torch.no_grad():\n",
    "            max_action = np.argmax(self.qnet_local(state).data.numpy())\n",
    "        self.qnet_local.train() # put net back in train mode\n",
    "        \n",
    "        rand_num = np.random.rand() # sample a random number uniformly between 0 and 1\n",
    "        \n",
    "        # implementing epsilon greedy policy\n",
    "        if rand_num > eps:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else: \n",
    "            return max_action\n",
    "        \n",
    "    def soft_update(self,tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(self.qnet_target.parameters(), self.qnet_local.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=brain.vector_observation_space_size, action_size=brain.vector_action_space_size)\n",
    "\n",
    "def dqn_train(max_episodes=1000,max_t=1000,eps_start=1.0,eps_end=1e-2,eps_decay=.995):\n",
    "        eps = eps_start\n",
    "        scores = []\n",
    "        score_window = deque(maxlen=100)\n",
    "        avg_scores = []\n",
    "        \n",
    "        for episode_count in range(max_episodes):\n",
    "            \n",
    "            # reset env state\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            state = env_info.vector_observations[0]\n",
    "            \n",
    "            done = env_info.local_done[0]\n",
    "            total_reward = 0\n",
    "            for t in range(max_t):\n",
    "                # choose action using the local q-network\n",
    "                action = agent.act(state,eps)\n",
    "\n",
    "                # taking action in the env\n",
    "                env_info = env.step(vector_action=action)[brain_name]\n",
    "\n",
    "                # getting next_state,reward,done from the env\n",
    "                next_state = env_info.vector_observations[0]\n",
    "                reward = env_info.rewards[0]\n",
    "                done = env_info.local_done[0]\n",
    "                \n",
    "                # using agent to perform a learning step and save the sample in the memory\n",
    "                agent.step(state,action,reward,next_state,done)\n",
    "                \n",
    "                # decay the epsilon value\n",
    "                eps = max(eps_decay*eps,eps_end)\n",
    "                \n",
    "                # get total reward for the episode\n",
    "                total_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "            scores.append(total_reward)\n",
    "            score_window.append(total_reward)\n",
    "            avg_scores.append(np.mean(score_window))\n",
    "\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode_count, np.mean(score_window)), end=\"\")\n",
    "            if episode_count % 100 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode_count, np.mean(score_window)))\n",
    "\n",
    "        return scores,avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: 0.00\n",
      "Episode 100\tAverage Score: 0.17\n",
      "Episode 141\tAverage Score: 0.26"
     ]
    }
   ],
   "source": [
    "scores,avg_scores = dqn_train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = Qnetwork(37,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = qnet(torch.from_numpy(state).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action.data.cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
