{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(\"Banana.app\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork(nn.Module):\n",
    "    def __init__(self,state_size,action_size):\n",
    "        # initialising the super class properties\n",
    "        super(Qnetwork,self).__init__()\n",
    "        \n",
    "        # defining layers\n",
    "        self.fc1 = nn.Linear(state_size,32)\n",
    "        self.fc2 = nn.Linear(32,64)\n",
    "        self.fc3 = nn.Linear(64,128)\n",
    "        self.out = nn.Linear(128,action_size)\n",
    "        \n",
    "    def forward(self,states):\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = F.relu(self.out(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from collections import deque,namedtuple\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,buffer_size,batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        e = self.experience(state,action,reward,next_state,done)\n",
    "        self.buffer.append(e)\n",
    "    \n",
    "    def sample(self,random=True):\n",
    "        if random:\n",
    "            choose = np.random.choice(range(len(self.buffer)),self.batch_size,replace=False)\n",
    "            experiences = [self.buffer[i] for i in choose]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,action_size,gamma=1.0,alpha=1e-3,lr=1e-3,\n",
    "                     buffer_size=10000,batch_size=64,tau=1e-3):\n",
    "        # defining local and target networks\n",
    "        self.qnet_local = Qnetwork(state_size,action_size).to(device)\n",
    "        self.qnet_target = Qnetwork(state_size,action_size).to(device)\n",
    "        \n",
    "        # set local and target parameters equal to each other\n",
    "        self.soft_update(tau=1.0)\n",
    "        \n",
    "        # experience replay buffer\n",
    "        self.memory = ReplayBuffer(buffer_size,batch_size)\n",
    "        \n",
    "        # defining variables\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.qnet_local.parameters(),lr=self.lr)\n",
    "    \n",
    "    def step(self,state,action,reward,next_state,done):\n",
    "        \"\"\" saves the step info in the memory buffer and perform a learning iteration\n",
    "        Input : \n",
    "            state,action,reward,state,done : non-batched numpy arrays\n",
    "        \n",
    "        Output : \n",
    "            none\n",
    "        \"\"\"\n",
    "        # add sample to the memory buffer\n",
    "        self.memory.add(state,action,reward,next_state,done)\n",
    "        \n",
    "        # use replay buffer to learn if it has enough samples\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "    \n",
    "        # run soft update\n",
    "        self.soft_update(self.tau)\n",
    "        \n",
    "    def learn(self,experiences):\n",
    "        \"\"\" perform a learning iteration by using sampled experience batch\n",
    "        Input : \n",
    "            experience : tuple from the memory buffer\n",
    "            states, actions, rewards, next_states, dones = experiences\n",
    "            eg : states.shape = [N,state_size]\n",
    "        Output : \n",
    "            none\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # predicted action value\n",
    "        q_pred = self.qnet_local.forward(states).gather(1,actions)\n",
    "        \n",
    "        # target action value\n",
    "        q_target = rewards + self.gamma*(1-dones)*self.qnet_target.forward(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # defining loss\n",
    "        loss = F.mse_loss(q_pred,q_target)\n",
    "        \n",
    "        # running backprop and optimizer step\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def act(self,state,eps=0.):\n",
    "        \"\"\" return the local model's predicted action for the given state\n",
    "        Input : \n",
    "            state : [state_size]\n",
    "        \n",
    "        Output : \n",
    "            action : scalar action as action space is discrete with dim = 1\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device) # converts numpy array to torch tensor\n",
    "        \n",
    "        self.qnet_local.eval() # put net in test mode\n",
    "        with torch.no_grad():\n",
    "            max_action = np.argmax(self.qnet_local(state).data.numpy())\n",
    "        self.qnet_local.train() # put net back in train mode\n",
    "        \n",
    "        rand_num = np.random.rand() # sample a random number uniformly between 0 and 1\n",
    "        \n",
    "        # implementing epsilon greedy policy\n",
    "        if rand_num > eps/self.action_size:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else: \n",
    "            return max_action\n",
    "        \n",
    "    def soft_update(self,tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(self.qnet_target.parameters(), self.qnet_local.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(37,4)\n",
    "def dqn_train(max_episodes=1000,eps_start=1.0,eps_end=1e-3,eps_decay=.99):\n",
    "        eps = eps_start\n",
    "        scores = []\n",
    "        score_window = deque(maxlen=100)\n",
    "        avg_scores = []\n",
    "        \n",
    "        for episode_count in range(max_episodes):\n",
    "            \n",
    "            # reset env state\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            state = env_info.vector_observations[0]\n",
    "            \n",
    "            done = env_info.local_done[0]\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                # choose action using the local q-network\n",
    "                action = agent.act(state,eps)\n",
    "\n",
    "                # taking action in the env\n",
    "                env_info = env.step(vector_action=action)[brain_name]\n",
    "\n",
    "                # getting next_state,reward,done from the env\n",
    "                next_state = env_info.vector_observations[0]\n",
    "                reward = env_info.rewards[0]\n",
    "                done = env_info.local_done[0]\n",
    "                \n",
    "                # using agent to perform a learning step and save the sample in the memory\n",
    "                agent.step(state,action,reward,next_state,done)\n",
    "                \n",
    "                # decay the epsilon value\n",
    "                eps = max(eps_decay*eps,eps_end)\n",
    "                \n",
    "                # get total reward for the episode\n",
    "                total_reward += reward\n",
    "                \n",
    "            scores.append(total_reward)\n",
    "            score_window.append(total_reward)\n",
    "            avg_scores.append(np.mean(score_window))\n",
    "\n",
    "            if episode_count%10 == 0 :\n",
    "                print (\"eps : {} ... score : {}\".format(episode_count,avg_scores[-1]))\n",
    "\n",
    "        return scores,avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps : 0 ... score : 0.0\n",
      "eps : 10 ... score : 0.5454545454545454\n",
      "eps : 20 ... score : 0.23809523809523808\n",
      "eps : 30 ... score : 0.1935483870967742\n",
      "eps : 40 ... score : 0.12195121951219512\n",
      "eps : 50 ... score : 0.09803921568627451\n",
      "eps : 60 ... score : 0.03278688524590164\n",
      "eps : 70 ... score : 0.028169014084507043\n",
      "eps : 80 ... score : 0.12345679012345678\n",
      "eps : 90 ... score : 0.10989010989010989\n",
      "eps : 100 ... score : 0.15\n",
      "eps : 110 ... score : 0.14\n",
      "eps : 120 ... score : 0.14\n",
      "eps : 130 ... score : 0.17\n",
      "eps : 140 ... score : 0.21\n",
      "eps : 150 ... score : 0.15\n",
      "eps : 160 ... score : 0.23\n",
      "eps : 170 ... score : 0.25\n",
      "eps : 180 ... score : 0.23\n",
      "eps : 190 ... score : 0.23\n",
      "eps : 200 ... score : 0.2\n",
      "eps : 210 ... score : 0.14\n",
      "eps : 220 ... score : 0.16\n",
      "eps : 230 ... score : 0.05\n",
      "eps : 240 ... score : 0.09\n",
      "eps : 250 ... score : 0.15\n",
      "eps : 260 ... score : 0.11\n",
      "eps : 270 ... score : 0.14\n",
      "eps : 280 ... score : 0.1\n",
      "eps : 290 ... score : 0.13\n",
      "eps : 300 ... score : 0.13\n",
      "eps : 310 ... score : 0.13\n",
      "eps : 320 ... score : 0.11\n",
      "eps : 330 ... score : 0.19\n",
      "eps : 340 ... score : 0.18\n",
      "eps : 350 ... score : 0.13\n",
      "eps : 360 ... score : 0.19\n",
      "eps : 370 ... score : 0.15\n",
      "eps : 380 ... score : 0.14\n",
      "eps : 390 ... score : 0.05\n",
      "eps : 400 ... score : 0.08\n",
      "eps : 410 ... score : 0.08\n",
      "eps : 420 ... score : 0.13\n",
      "eps : 430 ... score : 0.13\n",
      "eps : 440 ... score : 0.1\n",
      "eps : 450 ... score : 0.15\n",
      "eps : 460 ... score : 0.12\n",
      "eps : 470 ... score : 0.09\n",
      "eps : 480 ... score : 0.08\n",
      "eps : 490 ... score : 0.15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  -2.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  3.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  -1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  -3.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  -2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  3.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  2.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  -2.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  0.0,\n",
       "  -3.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  -2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  -2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  -2.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  -3.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  -3.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  -2.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  -2.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  -3.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  -2.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  -1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -2.0,\n",
       "  -2.0,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  -2.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.3333333333333333,\n",
       "  0.25,\n",
       "  0.6,\n",
       "  0.6666666666666666,\n",
       "  0.5714285714285714,\n",
       "  0.5,\n",
       "  0.5555555555555556,\n",
       "  0.5,\n",
       "  0.5454545454545454,\n",
       "  0.4166666666666667,\n",
       "  0.38461538461538464,\n",
       "  0.2857142857142857,\n",
       "  0.3333333333333333,\n",
       "  0.1875,\n",
       "  0.23529411764705882,\n",
       "  0.2222222222222222,\n",
       "  0.15789473684210525,\n",
       "  0.15,\n",
       "  0.23809523809523808,\n",
       "  0.22727272727272727,\n",
       "  0.34782608695652173,\n",
       "  0.3333333333333333,\n",
       "  0.28,\n",
       "  0.2692307692307692,\n",
       "  0.2222222222222222,\n",
       "  0.21428571428571427,\n",
       "  0.20689655172413793,\n",
       "  0.2,\n",
       "  0.1935483870967742,\n",
       "  0.1875,\n",
       "  0.18181818181818182,\n",
       "  0.17647058823529413,\n",
       "  0.17142857142857143,\n",
       "  0.16666666666666666,\n",
       "  0.10810810810810811,\n",
       "  0.07894736842105263,\n",
       "  0.1282051282051282,\n",
       "  0.125,\n",
       "  0.12195121951219512,\n",
       "  0.11904761904761904,\n",
       "  0.13953488372093023,\n",
       "  0.11363636363636363,\n",
       "  0.08888888888888889,\n",
       "  0.08695652173913043,\n",
       "  0.0851063829787234,\n",
       "  0.08333333333333333,\n",
       "  0.10204081632653061,\n",
       "  0.12,\n",
       "  0.09803921568627451,\n",
       "  0.07692307692307693,\n",
       "  0.09433962264150944,\n",
       "  0.09259259259259259,\n",
       "  0.07272727272727272,\n",
       "  0.05357142857142857,\n",
       "  0.03508771929824561,\n",
       "  0.034482758620689655,\n",
       "  0.03389830508474576,\n",
       "  0.03333333333333333,\n",
       "  0.03278688524590164,\n",
       "  0.04838709677419355,\n",
       "  0.06349206349206349,\n",
       "  0.046875,\n",
       "  0.03076923076923077,\n",
       "  0.030303030303030304,\n",
       "  -0.014925373134328358,\n",
       "  0.0,\n",
       "  0.028985507246376812,\n",
       "  0.02857142857142857,\n",
       "  0.028169014084507043,\n",
       "  0.05555555555555555,\n",
       "  0.0547945205479452,\n",
       "  0.08108108108108109,\n",
       "  0.10666666666666667,\n",
       "  0.11842105263157894,\n",
       "  0.12987012987012986,\n",
       "  0.14102564102564102,\n",
       "  0.12658227848101267,\n",
       "  0.1375,\n",
       "  0.12345679012345678,\n",
       "  0.10975609756097561,\n",
       "  0.12048192771084337,\n",
       "  0.11904761904761904,\n",
       "  0.11764705882352941,\n",
       "  0.12790697674418605,\n",
       "  0.11494252873563218,\n",
       "  0.10227272727272728,\n",
       "  0.12359550561797752,\n",
       "  0.12222222222222222,\n",
       "  0.10989010989010989,\n",
       "  0.10869565217391304,\n",
       "  0.11827956989247312,\n",
       "  0.11702127659574468,\n",
       "  0.12631578947368421,\n",
       "  0.13541666666666666,\n",
       "  0.13402061855670103,\n",
       "  0.14285714285714285,\n",
       "  0.1414141414141414,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.11,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.16,\n",
       "  0.17,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.17,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.1,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.17,\n",
       "  0.17,\n",
       "  0.17,\n",
       "  0.17,\n",
       "  0.16,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.18,\n",
       "  0.18,\n",
       "  0.21,\n",
       "  0.23,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.21,\n",
       "  0.23,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.2,\n",
       "  0.2,\n",
       "  0.18,\n",
       "  0.18,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.17,\n",
       "  0.17,\n",
       "  0.17,\n",
       "  0.18,\n",
       "  0.19,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.24,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.26,\n",
       "  0.26,\n",
       "  0.28,\n",
       "  0.28,\n",
       "  0.26,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.24,\n",
       "  0.24,\n",
       "  0.22,\n",
       "  0.19,\n",
       "  0.19,\n",
       "  0.19,\n",
       "  0.18,\n",
       "  0.2,\n",
       "  0.21,\n",
       "  0.23,\n",
       "  0.25,\n",
       "  0.23,\n",
       "  0.23,\n",
       "  0.23,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.23,\n",
       "  0.24,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.21,\n",
       "  0.18,\n",
       "  0.19,\n",
       "  0.2,\n",
       "  0.2,\n",
       "  0.21,\n",
       "  0.2,\n",
       "  0.17,\n",
       "  0.18,\n",
       "  0.18,\n",
       "  0.16,\n",
       "  0.17,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.13,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.14,\n",
       "  0.17,\n",
       "  0.19,\n",
       "  0.18,\n",
       "  0.16,\n",
       "  0.16,\n",
       "  0.16,\n",
       "  0.13,\n",
       "  0.12,\n",
       "  0.1,\n",
       "  0.08,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.13,\n",
       "  0.12,\n",
       "  0.11,\n",
       "  0.11,\n",
       "  0.11,\n",
       "  0.09,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.12,\n",
       "  0.11,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.09,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.17,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.17,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.12,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.1,\n",
       "  0.12,\n",
       "  0.11,\n",
       "  0.13,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.17,\n",
       "  0.19,\n",
       "  0.19,\n",
       "  0.19,\n",
       "  0.19,\n",
       "  0.19,\n",
       "  0.2,\n",
       "  0.19,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.2,\n",
       "  0.19,\n",
       "  0.18,\n",
       "  0.17,\n",
       "  0.18,\n",
       "  0.17,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.19,\n",
       "  0.21,\n",
       "  0.2,\n",
       "  0.19,\n",
       "  0.18,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.13,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.12,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.16,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.17,\n",
       "  0.15,\n",
       "  0.12,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.05,\n",
       "  0.04,\n",
       "  0.04,\n",
       "  0.05,\n",
       "  0.07,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.06,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.07,\n",
       "  0.07,\n",
       "  0.07,\n",
       "  0.07,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.07,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.08,\n",
       "  0.1,\n",
       "  0.09,\n",
       "  0.1,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.12,\n",
       "  0.1,\n",
       "  0.13,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.11,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.09,\n",
       "  0.1,\n",
       "  0.12,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.09,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.15,\n",
       "  0.17,\n",
       "  0.18,\n",
       "  0.18,\n",
       "  0.17,\n",
       "  0.16,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.12,\n",
       "  0.09,\n",
       "  0.11,\n",
       "  0.15,\n",
       "  0.14,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.09,\n",
       "  0.07,\n",
       "  0.07,\n",
       "  0.08,\n",
       "  0.1,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.12,\n",
       "  0.11,\n",
       "  0.09,\n",
       "  0.08,\n",
       "  0.11,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.11,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.16,\n",
       "  0.17,\n",
       "  0.15,\n",
       "  0.13,\n",
       "  0.12,\n",
       "  0.1,\n",
       "  0.09,\n",
       "  0.09,\n",
       "  0.09,\n",
       "  0.1,\n",
       "  0.08,\n",
       "  0.08])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores,avg_scores = dqn_train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = Qnetwork(37,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = qnet(torch.from_numpy(state).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action.data.cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
