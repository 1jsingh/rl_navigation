{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(\"Banana.app\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork(nn.Module):\n",
    "    def __init__(self,state_size,action_size):\n",
    "        # initialising the super class properties\n",
    "        super(Qnetwork,self).__init__()\n",
    "        \n",
    "        # defining layers\n",
    "        self.fc1 = nn.Linear(state_size,32)\n",
    "        self.fc2 = nn.Linear(32,64)\n",
    "        self.fc3 = nn.Linear(64,128)\n",
    "        self.out = nn.Linear(128,action_size)\n",
    "        \n",
    "    def forward(self,states):\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = F.relu(self.out(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self,buffer_size,batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        e = self.experience(state,action,reward,next_state,done)\n",
    "        self.buffer.append(e)\n",
    "    \n",
    "    def sample(self,random=True):\n",
    "        if random:\n",
    "            choose = np.random.choice(range(len(self.buffer)),self.batch_size,replace=False)\n",
    "            return [self.buffer[i] for i in choose]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,action_size,gamma=1.0,alpha=1e-3,lr=1e-3,\n",
    "                     buffer_size=1e4,batch_size=64):\n",
    "        # defining local and target networks\n",
    "        self.qnet_local = Qnetwork(state_size,action_size)\n",
    "        self.qnet_target = Qnetwork(state_size,action_size)\n",
    "        \n",
    "        # experience replay buffer\n",
    "        self.memory = ReplayBuffer(buffer_size,batch_size)\n",
    "        \n",
    "        # defining variables\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.qnet_local.parameters(),lr=self.lr)\n",
    "    \n",
    "    def step(self,state,action,reward,next_state,done):\n",
    "        # add sample to the memory buffer\n",
    "        self.memory.add(state,action,reward,next_state,done)\n",
    "        \n",
    "        # use replay buffer to learn if it has enough samples\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "    \n",
    "    def learn(self,experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # predicted action value\n",
    "        q_pred = self.qnet_local(states)\n",
    "        \n",
    "        # target action value\n",
    "        q_target = rewards + self.gamma*(1-dones)*self.qnet_target(next_states)[actions]\n",
    "        \n",
    "        # defining loss\n",
    "        loss = F.mse_loss(q_pred,q_target)\n",
    "        \n",
    "        # running backprop and optimizer step\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "    def act(self,state):\n",
    "        action = self.model.forward(state)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-59d8f3fad3b4>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-59d8f3fad3b4>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    reward = env_info.\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def dqn_train(max_episodes):\n",
    "    \n",
    "        for eps in range(max_episodes):\n",
    "            \n",
    "            # reset env state\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            state = env_info.vector_observation[0]\n",
    "            \n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            while not done:\n",
    "                # choose action using the local q-network\n",
    "                action = agent.act(state)\n",
    "\n",
    "                # taking action in the env\n",
    "                env_info = env.step(vector_action=action)[brain_name]\n",
    "\n",
    "                # getting next_state,reward,done from the env\n",
    "                next_state = env_info.vector_observation[0]\n",
    "                reward = env_info.rewards[0]\n",
    "                done = env_info.local_done[0]\n",
    "                \n",
    "                # using agent to perform a learning step and save the sample in the memory\n",
    "                agent.step(state,action,reward,next_state,done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BananaBrain': <unityagents.brain.BrainParameters at 0x10cfe9e80>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
